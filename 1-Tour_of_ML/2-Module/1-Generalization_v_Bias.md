
# ğŸ“˜ Generalization Error in Regression â€“ Study Notes

This guide explains how machine learning models handle **regression problems** and how we evaluate their performance using the concept of **generalization error**.

---

## ğŸ§  What is Regression?

**Regression** is when we try to predict a **number** based on input data.  
Examples:
- Predicting a house price from its size and location
- Predicting a stock price based on its history

---

## ğŸ§ª The Regression Model

We assume that:

```
y = f(x) + Îµ
```

Where:
- `x` = input features
- `f(x)` = the true function (unknown to us)
- `Îµ` = random noise (real-world randomness)

---

## ğŸ“Š Noise Assumptions

```
E[Îµ] = 0
E[ÎµÂ²] = ÏƒÂ²
```

- The noise has **zero average** (no bias)
- The **variance** (spread) of the noise is `ÏƒÂ²`

This leads to:

```
E[y] = f(x)
Var(y) = ÏƒÂ²
```

---

## ğŸ¯ Goal: Learn an Approximation of f(x)

Since we donâ€™t know the true function `f(x)`, we try to learn an approximation, `Æ’Ì‚(x)`, using a machine learning model.

### Our Objective:
Minimize the **generalization error**:

```
E[(y - Æ’Ì‚(x))Â²]
```

This is the **expected squared error** over both training and unseen data.

---

## ğŸ” Error Decomposition

The generalization error can be expanded as:

```
E[(y - Æ’Ì‚(x))Â²] = E[yÂ²] + E[Æ’Ì‚Â²] - 2E[y Æ’Ì‚]
```

This helps us break the error into parts we can analyze.

---

## ğŸ’¡ Summary

| Term                      | Meaning                                           |
|---------------------------|---------------------------------------------------|
| `f(x)`                    | The real function generating `y`                 |
| `Æ’Ì‚(x)`                    | The modelâ€™s approximation of `f(x)`              |
| `Îµ`                       | Random noise                                      |
| Generalization Error      | Average prediction error across all data         |
| Goal of Learning          | **Generalize** â€“ perform well on **unseen** data |

---

## ğŸ§  What is the Bias-Variance Decomposition?

When we train a model to predict a number (e.g., house price), we measure how far off the model is using:

```
E[(y - Æ’Ì‚(x))Â²]
```

This is the **average squared error** between the true value `y` and our modelâ€™s prediction `Æ’Ì‚(x)`.

---

## ğŸ§ª Total Error Breakdown

This total error can be broken down into three parts:

```
Error = (bias)Â² + variance + noise
```

---

## ğŸ” Components Explained

### ğŸ¯ BiasÂ²
**Bias** is how far, on average, your model's predictions are from the truth.

- High bias means the model is too simple and **underfits** the data.
- Example: Using a straight line to fit curved data.

---

### ğŸŒŠ Variance
**Variance** measures how much the modelâ€™s predictions change depending on the training data.

- High variance means the model is too sensitive and **overfits** the training data.
- Example: A model that memorizes training data but fails on new data.

---

### ğŸ”Š Noise (ÏƒÂ²)
**Noise** is the random variation in the data that **cannot be explained** â€” it's built into the world.

- Even a perfect model can't eliminate noise.
- Noise is **unavoidable** and **not your modelâ€™s fault**.

---

## ğŸ§¾ Summary Table

| Component   | What it Means                                                | Can We Reduce It?           |
|-------------|--------------------------------------------------------------|------------------------------|
| Bias        | Systematic error due to overly simple models (underfitting)  | âœ… Yes â€” use better models   |
| Variance    | Error from overly complex models reacting too much           | âœ… Yes â€” use regularization  |
| Noise       | Randomness in real-world data                                | âŒ No â€” it's out of our control |

---

## âš–ï¸ Why This Matters

- A model that is **too simple** has high bias.
- A model that is **too complex** has high variance.
- The best model **balances both** â€” a sweet spot where total error is minimized.

---

